# app.py
from __future__ import annotations
import os, glob, pandas as pd, streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ I/O helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
@st.cache_data(show_spinner=False)
def load_csv_tree(root: str, *, exclude: str | None = None) -> pd.DataFrame:
    pat = os.path.join(root, "**/*.csv")
    files = [f for f in glob.glob(pat, recursive=True) if not exclude or exclude not in f]
    return pd.concat((pd.read_csv(f) for f in files), ignore_index=True)

@st.cache_data(show_spinner=False)
def load_translation(root: str) -> pd.DataFrame:
    return pd.read_csv(os.path.join(root, "translation.csv"))

def id_to_label(ids, trans_df, lang):
    mapping = (
        trans_df.loc[trans_df["Lang"] == lang]
        .set_index(trans_df.columns[0])["Label"]
        .to_dict()
    )
    return [mapping.get(i, i) for i in ids]

def label_to_id(labels, trans_df, lang):
    mapping = (
        trans_df.loc[trans_df["Lang"] == lang]
        .set_index("Label")[trans_df.columns[0]]
        .to_dict()
    )
    return [mapping[lbl] for lbl in labels if lbl in mapping]

def make_mask(link_df, key_col, picked_ids, logic) -> set[str]:
    """
    Generate a set of IDs based on filtering logic applied to a DataFrame.

    Args:
        link_df (pd.DataFrame): The input DataFrame containing the data to filter.
        key_col (str): The column name in the DataFrame to apply the filtering logic on.
        picked_ids (Iterable): A collection of IDs to filter against.
        logic (str): A string specifying the filtering logic. If it starts with "ã™ã¹ã¦",
                     the function checks if all `picked_ids` are a subset of the values
                     in `key_col` grouped by "MishearID". Otherwise, it filters rows
                     where `key_col` contains any of the `picked_ids`.

    Returns:
        set[str]: A set of "MishearID" values that match the filtering criteria.
    """
    if not picked_ids:
        return set(link_df["MishearID"])
    if logic.startswith("ã™ã¹ã¦"):
        # FIXME: ã€Œã™ã¹ã¦ã€ã¨ã„ã†ã®ã¯radioã«ä¾å­˜ã—ã¦ã„ã‚‹
        ok = link_df.groupby("MishearID")[key_col].apply(lambda s: set(picked_ids).issubset(s))
        return set(ok[ok].index)
    return set(link_df[link_df[key_col].isin(picked_ids)]["MishearID"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Core application class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
class MishearingApp:
    ROOT = "data"

    def __init__(self):
        tag_root = f"{self.ROOT}/tag"
        env_root = f"{self.ROOT}/environment"
        mishear_root = f"{self.ROOT}/mishearing"

        self.tag_trans = load_translation(tag_root)
        self.env_trans = load_translation(env_root)

        self.tag_link = load_csv_tree(tag_root, exclude="translation.csv")
        self.env_link = load_csv_tree(env_root, exclude="translation.csv")
        self.corpus   = load_csv_tree(mishear_root)

        # Pre-compute counts
        self.tag_counts = self.tag_link["TagID"].value_counts()
        self.env_counts = self.env_link["EnvID"].value_counts()

    # ------------- UI ------------ #
    def sidebar_form(self):
        with st.sidebar:
            st.title("ğŸ” Filter")
            lang = st.radio("Language", ("en", "ja"), horizontal=True, index=1)
            with st.form(key="filter_form"):
                tag_lbls = id_to_label(self.tag_counts.index, self.tag_trans, lang)
                env_lbls = id_to_label(self.env_counts.index, self.env_trans, lang)

                picked_tags = st.multiselect("Tags", tag_lbls)
                tag_logic   = st.radio("Tag rule", ["ã™ã¹ã¦å«ã‚€ (AND)", "ã„ãšã‚Œã‹å«ã‚€ (OR)"])

                picked_envs = st.multiselect("Environments", env_lbls)
                env_logic   = st.radio("Env rule", ["ã™ã¹ã¦å«ã‚€ (AND)", "ã„ãšã‚Œã‹å«ã‚€ (OR)"])

                submitted = st.form_submit_button("Apply filters")

            return submitted, lang, picked_tags, tag_logic, picked_envs, env_logic

    def run(self):
        submitted, lang, p_tag_lbl, tag_logic, p_env_lbl, env_logic = self.sidebar_form()
        if not submitted:
            st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ•ã‚£ãƒ«ã‚¿ã‚’é¸ã‚“ã§ **Apply filters** ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            return

        # --- translate back to IDs --- #
        p_tag_ids = label_to_id(p_tag_lbl, self.tag_trans, lang)
        p_env_ids = label_to_id(p_env_lbl, self.env_trans, lang)

        keep_tag = make_mask(self.tag_link, "TagID", p_tag_ids, tag_logic)
        keep_env = make_mask(self.env_link, "EnvID", p_env_ids, env_logic)
        final_ids = keep_tag & keep_env

        # --- main pane --- #
        st.header(f"Results â€“ {len(final_ids)} rows")
        st.dataframe(self.corpus[self.corpus["MishearID"].isin(final_ids)])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bootstrap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

def main():
    st.title("Mishearing Corpus Viewer")
    MishearingApp().run()

st.set_page_config(page_title="Mishearing Corpus", layout="wide")
main_tab, google_tab, scraping_tab, loop_tab, fix_csv_tab = st.tabs(["main", "google", "scraping", "loop", "fix_csv"])

with main_tab:
    main()

import requests
import json


@st.cache_data(show_spinner=False)
def scrape(target_url, save_path):
    input_str = json.dumps({"url": target_url, "save_dir": save_path})
    st.write(input_str)
    url = "http://127.0.0.1:7860/api/v1/run/cbda4a09-af9d-41b7-8376-232e50b75e3f"  # The complete API endpoint URL for this flow
    # Request payload configuration
    payload = {
        "input_value": input_str,  # The input value to be processed by the flow
        "output_type": "chat",  # Specifies the expected output format
        "input_type": "text"  # Specifies the input format
    }

    # Request headers
    headers = {
        "Content-Type": "application/json"
    }

    try:
        # Send API request
        response = requests.request("POST", url, json=payload, headers=headers)
        response.raise_for_status()  # Raise exception for bad status codes

        # Print response
        try:
            response_text = response.text  # Get response text
            response_json = json.loads(response_text)  # Parse text as JSON
            st.json(response_json)  # Display the JSON in a formatted way
        except json.JSONDecodeError as e:
            st.error(f"Error parsing response text as JSON: {e}")

    except requests.exceptions.RequestException as e:
        st.error(f"Error making API request: {e}")
    except ValueError as e:
        st.error(f"Error parsing response: {e}")
    

with scraping_tab:
    # Scrapeã®å¯¾è±¡ã®URL
    target_url = st.text_input("URL: ", "https://note.com/imominty/n/ne9a96dee1118")
    save_path = st.text_input("base path: ", "/home/kishiyamat/mishearing-corpus/directory")
    if not (target_url and save_path):
        st.warning("Please enter a URL to scrape and Directory to save it to.")
    else:
        if st.button("Save CSV"):
            scrape(target_url, save_path)


from apify_client import ApifyClient

# â”€â”€â”€ joblib å°å…¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pathlib import Path
from joblib import Memory
from apify_client import ApifyClient
import streamlit as st

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç½®ãå ´æ‰€ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ .cache/ ã«è¨­å®š
memory = Memory(location=Path(".cache"), verbose=0)

@memory.cache                        # â† st.cache_resource ã‚’ç½®ãæ›ãˆ
def run_apify_actor(
    queries: str,
    results_per_page: int,
    max_pages_per_query: int,
):
    """Apify Google Search Scraper ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ joblib ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    # æ—¥ä»˜ã¨ã‹ã‚‚æœ¬å½“ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãŸã»ã†ãŒè‰¯ã„.

    client = ApifyClient(
        st.secrets["apify"]["token"],
        max_retries = 10,
        min_delay_between_retries_millis = 2000,
        timeout_secs = 360,
    )

    run_input = {
        "queries": queries,
        "resultsPerPage": results_per_page,
        "maxPagesPerQuery": max_pages_per_query,
        "countryCode": "jp",
        "languageCode": "ja",
        "forceExactMatch": True,
    }

    # Actor å®Ÿè¡Œ
    run = client.actor("apify/google-search-scraper").call(run_input=run_input, wait_secs=360)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—
    dataset = client.dataset(run["defaultDatasetId"])

    # Apify ã® Dataset ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãã®ã¾ã¾ã§ã¯ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã§ããªã„
    # list_items() ã§ JSON äº’æ›ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‹ã‚‰è¿”ã™
    return list(dataset.iterate_items())


with google_tab:
    st.write("This tab is for Apify integration. Currently, it is not implemented.")
    queries = st.text_input("Google Search Query", '"ã¨*ã®èãé–“é•"')
    results_per_page = st.number_input("results_per_page", 10)
    max_pages_per_query = st.number_input("max_pages_per_query (1ã—ã‹å‹•ã‹ãªã„)",1)
    if not queries:
        st.warning("Please enter a query to run the Apify Actor.")
    else: 
        if st.button("Run Apify Actor"):
            # Initialize the ApifyClient with your Apify API token
            # Replace '<YOUR_API_TOKEN>' with your token.
            run_input = {
                "queries": queries,
                "results_per_page": results_per_page,
                "max_pages_per_query": max_pages_per_query,
            }
            dataset = run_apify_actor(**run_input)
            items = []
            for item in dataset:
                items.extend(item["organicResults"]) 
            st.write(len(items))
            # You can add your Apify-related code here if needed.
            # organicResults ã«æ¤œç´¢çµæœãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ã€ãã“ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

with loop_tab:
    st.write("This tab is for Apify integration. Currently, it is not implemented.")
    queries = st.text_input("Query to apply loop")
    save_path = st.text_input("base path (Loop): ", "/home/kishiyamat/mishearing-corpus/directory")
    results_per_page = st.number_input("results_per_page (loop)", 10)
    max_pages_per_query = st.number_input("max_pages_per_query (loop)",1)
    if not queries:
        st.warning("Please enter a query to run the Apify Actor.")
    else: 
        if st.button("Run Apify Actor Loop"):
            # Initialize the ApifyClient with your Apify API token
            # Replace '<YOUR_API_TOKEN>' with your token.
            run_input = {
                "queries": queries,
                "results_per_page": results_per_page,
                "max_pages_per_query": max_pages_per_query,
            }
            dataset = run_apify_actor(**run_input)
            for item in dataset:
                for organic_result in item["organicResults"]:
                    st.write(organic_result["url"])
                    scrape(organic_result["url"], save_path)
            # You can add your Apify-related code here if needed.
            # organicResults ã«æ¤œç´¢çµæœãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ã€ãã“ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

# fix_csv_tab.py
import json
from pathlib import Path

import requests
import streamlit as st

API_URL_FIX_CSV = (
    "http://127.0.0.1:7860/api/v1/run/"
    "688463f5-255f-4368-88f9-e3fb5ed17a50"
)

import pandas as pd

from io import StringIO

with fix_csv_tab:
    """è¤‡æ•° CSV ã‚’ API ã«é€ã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.header("CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£ã—ã¦ API ã¸é€ä¿¡")
    # â–¼ 1. è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚‹
    save_path = st.text_input("Path where files to be fixed exist")
    save_dir_fixed = st.text_input("Path where fixed files to be fixed exist")
    save_dir_envs = st.text_input("Path where envs files to be fixed exist")
    save_dir_tags = st.text_input("Path where tags files to be fixed exist")
    files = st.file_uploader(
        "CSV ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰", type="csv", accept_multiple_files=True
    )

    # â–¼ 2. ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¾ã§å‡¦ç†ã‚’å¾…æ©Ÿ
    # è¡Œæ•°ãŒåŒã˜ã“ã¨ã¯ä¿è¨¼ã—ãŸã„ã€‚
    if files and st.button("é€ä¿¡"):
        for f in files:
            st.write(f)
            # 2-A. ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å–å¾—
            csv_text = f.getvalue().decode("utf-8")

            csvStringIO = StringIO(csv_text)
            df = pd.read_csv(csvStringIO, sep=",", header=0)
            original_nrow = len(df)
            st.write(df)

            # 2-C. JSON æ–‡å­—åˆ—ã‚’ä½œæˆ
            payload_input = {
                "save_path": save_path,
                "fixed_dir": save_dir_fixed,
                "envs_dir": save_dir_envs,
                "tags_dir": save_dir_tags,
                "csv_name": f.name,
                "json_text": df.to_json(orient="records", force_ascii=False),
            }
            payload_input_str = json.dumps(payload_input, ensure_ascii=False)
            payload = {
                "input_value": payload_input_str,  # The input value to be processed by the flow
                "output_type": "chat",  # Specifies the expected output format
                "input_type": "text"  # Specifies the input format
            }
            # Request headers
            headers = {
                "Content-Type": "application/json"
            }

            # 2-D. é€ä¿¡å†…å®¹ã‚’ç”»é¢ã«è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            st.code(payload_input_str, language="json")

            # 2-E. API ã¸ POST
            try:
                r = requests.request("POST", API_URL_FIX_CSV, json=payload, headers=headers)
                r.raise_for_status()
            except requests.RequestException as e:
                st.error(f"API Error: {e}")
                continue  # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸é€²ã‚€

            new_df = pd.read_csv(f"{save_path}/{save_dir_fixed}/{f.name}")
            st.write(new_df)
            assert original_nrow==len(new_df)
            # ã“ã“ã§å‡ºåŠ›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æ­£ã—ã•ã‚’ä¿è¨¼
            # 2-F. çµæœã‚’è¡¨ç¤º
            st.success(f"{f.name} â†’ æˆåŠŸ")
            st.json(r.json())
